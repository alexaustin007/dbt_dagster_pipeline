# ğŸ¬ Retailâ€¯Dataâ€¯Pipeline â€” LocalÂ Batchâ€¯+â€¯StreamingÂ ETL

A fullyâ€‘local dataâ€‘engineering sandbox that lets you practise **endâ€‘toâ€‘end pipelines** using openâ€‘source tools only:

| Layer            | Tool(s)                               | Role |
|------------------|---------------------------------------|------|
| Storage          | **MySQLâ€¯8** (local)                   | Raw, staging & marts |
| Batch Ingestion  | **Pythonâ€¯+â€¯pandas**                   | CSV / API â†’ MySQL |
| Batch Processing | **PySpark** (optional)                | Heavy cleans / dedupe |
| Transformations  | **dbtâ€‘mysql**                         | Stagingâ€¯â†’â€¯Marts |
| Orchestration    | **Dagster**                           | Run & monitor jobs |
| Streaming Demo   | **Kafka â–¸ Sparkâ€¯Structuredâ€¯Streaming**| Realâ€‘time events |
| Exploration      | **JupyterÂ Notebook** / BI tool        | Adâ€‘hoc queries |

---

## ğŸ“‚ DirectoryÂ Layout (highâ€‘level)

```text
retail_data_pipeline/
â”œâ”€ data/                     # â‡¢ Drop raw CSV/JSON files here
â”‚  â”œâ”€ raw/
â”‚  â””â”€ processed/
â”œâ”€ ingestion/                # Python loaders (file & API)
â”œâ”€ spark_jobs/               # Batch Spark scripts
â”œâ”€ streaming/                # Kafka producer + Spark stream
â”œâ”€ mysql/                    # Oneâ€‘off init SQL for known tables
â”œâ”€ dbt_project/              # dbt models (staging â†’ marts)
â”œâ”€ dagster_project/          # Dagster jobs & repo
â”œâ”€ notebooks/                # Analysis / EDA
â”œâ”€ requirements.txt          # Python deps
â”œâ”€ .env                      # DB creds & config
â””â”€ PROJECT_OVERVIEW.md       # â† you are here
ğŸ”„ Endâ€‘toâ€‘EndÂ Flow
1â€¯â–ªâ€¯Batch ETL (default)

data/raw/*.csv
        â”‚
        â–¼  (pandas infers schema)
ingestion/from_file.py
        â”‚   stg_<file>.csv  â† autoâ€‘created
        â–¼
      MySQL  â”€â”€â”
               â”‚ dbtÂ run
               â–¼
      MySQL (marts)  â†’ notebooks / BI
2â€¯â–ªâ€¯Streaming Demo (optional)

Kafka producer  â†’ topic=sales_events
        â–¼
SparkÂ StructuredÂ Streaming
        â–¼
MySQL.stream_sales_events
        â–¼  dbtÂ run -m fct_streaming_sales
MySQL.fct_streaming_sales (hourly rolls)
Dagster drives both pipelines:


dagster dev  â†’  retail_pipeline  (batch)
            â†³  streaming_pipeline (stream + rollâ€‘up)
ğŸš€ QuickÂ Start
Install prerequisites


# PythonÂ 3.9+, JavaÂ 8+, MySQLÂ 8+
pip install -r requirements.txt
Create & seed MySQL


mysql -u root -p < mysql/init_schema.sql
mysql -u root -p < mysql/init_stream_schema.sql   # when streaming
Batch ingest CSVs


# place sales.csv in data/raw/
python ingestion/from_file.py --db_user root --db_password <pw>
Run dbt models


cd dbt_project
dbt run
dbt test
Start Dagster UI


dagster dev -f dagster_project/repository.py
# kick off `retail_pipeline` from the UI
Streaming demo (optional)

Start Kafka locally (e.g., bin/kafka-server-start.sh â€¦).

Produce events:


python streaming/kafka_producer.py
In another terminal run the Spark stream job:


spark-submit streaming/spark_stream_job.py
Trigger the Dagster streaming_pipeline to roll up hourly metrics.

ğŸ›  Configuration
File	Purpose
.env	MySQL host/user/pw for local runs
dbt_project/profiles.yml	dbt connection profile
dagster_project/dagster.yaml	Dagster instance settings


additionally install everything inside a python virtual environment of venv
/Users/alexaustinchettiar/Downloads/retail_data_pipeline_full

